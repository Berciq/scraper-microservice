# Scraper Microservice

This is a microservice composed of an API and a webscraper. Use the API to schedule scrape jobs in order to download text and images from webpages.

The API is designed using the OpenAPI specification (previusly Swagger) and it includes the Swagger UI which simplifies working with the API.

Simply build and run the Docker application and access the Swagger UI on port 5000 by default. The UI will tell you everything there is to know about the API.

## Author's Comments

I chose to implement the API part of the microservice using Flask + Swagger because it seemed like a perfect combination to implement a solid, reliable, and self-documented API. I also concluded that the Swagger UI would be a big bonus for anyone having to learn to use this API since it allows for playing around with the API without writing any code, directly in the browser. For the web scraping part of the microservice, I decided to use Scrapy because it is a mature, stable, and efficient framework (built on top of Twisted) for scraping content from the web. Having these two services separated out this way (the API and the scraper) seems to fit well with the microservice architecture because both services can be extended and scaled independently. 

Because this project was so simple, I decided to have the two services communicate with each other just by storing and reading information to and from the database. As an example, the API stores scrape jobs in the database. The scraper polls the database each time it goes idle and starts working on any new scrape jobs. As the scraper asynchronously processes the web pages, it stores the results to the database, which are then accessible by the API. I think this solution is fine, but in retrospect, a more robust solution would have been to use a task queue and a message broker such as Celery + RabbitMQ to allow the two services to talk to each other. This could still be implemented rather easily.

One design choice I made early on was to use unique identifiers for each scrape job rather than the URLs. URLs can be long and tricky to work with if you make them the resource identifier in a REST API. A hash of the URL could be used as a resource identifier but hashes are not a good solution here because of potential collisions. The problem with a unique identifier, though, is that you could end up needlessly scraping the same web page multiple times if this was requested several times by different users for example. To mitigate this, I included a parameter in the API that gives the user control over whether a scrape job request should be scheduled as a new job or whether the user would like to receive the identifier for the most recent scheduled scrape job for the given URL if there is one.

Another design choice I made had to do with storing images. One solution would have been to store the images in the database. This didn't seem most appropriate so I decided to store the images in the filesystem in a way that could be extended to use an external storage container. The images downloaded as part of a single scrape job are stored in a directory named after the scrape job ID. This is a fairly "flat" structure that can lead to problems when the number of scrape jobs becomes very large and the number of directories in the storage grows significantly. To mitigate this, the directory structure could be modified such that the images are stored in a series of subfolders. The subfolder names could be based off of a hash of the scrape job ID, split into chunks, where each chunk is a subsequent subdirectory name. This would guarantee a more robust folder structure. Nevertheless, given how simple this service is at the moment, this doesn't seem like a real concern.

Lastly, I decided that it would be beneficial to anyone using the API if all images were converted to a common format. I decided to use the JPEG format. The capability to convert the images was already built into scrapy, so this was easy to implement. I also assumed that most users would want to get both the text and the images from a web page. So in my API I included only one endpoint for scheduling a scrape job. By default this results in both the text and the images being scraped, but this can be controlled through the parameters submitted as part of the API call.

For more details on the API please refer to the app's Swagger UI. I tried to document the API in detail so the UI is rich in information.
